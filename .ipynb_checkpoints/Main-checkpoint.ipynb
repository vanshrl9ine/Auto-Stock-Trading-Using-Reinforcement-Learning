{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc709d59-3ab6-401e-a803-4ff31993ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "\n",
    "from finrl import config\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfed73f-b71d-450e-8532-471fc33abde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_symbols = [\n",
    "#     'ADANIENT', 'ADANIPORTS', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK',\n",
    "#     'BAJAJ-AUTO', 'BAJFINANCE', 'BAJAJFINSV', 'BPCL', 'BHARTIARTL',\n",
    "#     'BRITANNIA', 'CIPLA', 'COALINDIA', 'DIVISLAB', 'DRREDDY', 'EICHERMOT',\n",
    "#     'GRASIM', 'HCLTECH', 'HDFCBANK', 'HDFCLIFE', 'HEROMOTOCO', 'HINDALCO',\n",
    "#     'HINDUNILVR', 'ICICIBANK', 'INDUSINDBK', 'INFY', 'ITC', 'JSWSTEEL',\n",
    "#     'KOTAKBANK', 'LT', 'LTIM', 'M&M', 'MARUTI', 'NESTLEIND', 'NTPC', 'ONGC',\n",
    "#     'POWERGRID', 'RELIANCE', 'SBILIFE', 'SBIN', 'SUNPHARMA', 'TATAMOTORS',\n",
    "#     'TATASTEEL', 'TCS', 'TATACONSUM', 'TECHM', 'TITAN', 'ULTRACEMCO', 'UPL',\n",
    "#     'WIPRO'\n",
    "# ]\n",
    "\n",
    "# ns_company_symbols = [symbol + '.NS' for symbol in company_symbols]\n",
    "\n",
    "# print(ns_company_symbols)\n",
    "# symbols=ns_company_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09ec60f-d1f4-4b21-97a6-60fe0c7cc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2020-07-01'\n",
    "TRADE_START_DATE = '2020-07-01'\n",
    "TRADE_END_DATE = '2023-05-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70746a8e-df0e-49b5-a146-21dab4e23a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                                 end_date = TRADE_END_DATE,\n",
    "#                                 ticker_list = symbols).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b33b42-1022-48ba-9bb4-724b1fa70c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_csv('datasets/BSE30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5e44b-1124-4426-87de-925b8c8fff98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19eecd6d-8e85-4f7e-a6b3-542e23cab110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105703 entries, 0 to 105702\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    105703 non-null  object \n",
      " 1   open    105703 non-null  float64\n",
      " 2   high    105703 non-null  float64\n",
      " 3   low     105703 non-null  float64\n",
      " 4   close   105703 non-null  float64\n",
      " 5   volume  105703 non-null  int64  \n",
      " 6   tic     105703 non-null  object \n",
      " 7   day     105703 non-null  int64  \n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9b1894-7f81-4c66-958f-2932b8cc5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end, target_date_col=\"date\"):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df[target_date_col] >= start) & (df[target_date_col] < end)]\n",
    "    data = data.sort_values([target_date_col, \"tic\"], ignore_index=True)\n",
    "    data.index = data[target_date_col].factorize()[0]\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_datetime(time):\n",
    "    time_fmt = \"%Y-%m-%dT%H:%M:%S\"\n",
    "    if isinstance(time, str):\n",
    "        return datetime.datetime.strptime(time, time_fmt)\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Provides methods for preprocessing the stock price data\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        use_technical_indicator : boolean\n",
    "            we technical indicator or not\n",
    "        tech_indicator_list : list\n",
    "            a list of technical indicator names (modified from neofinrl_config.py)\n",
    "        use_turbulence : boolean\n",
    "            use turbulence index or not\n",
    "        user_defined_feature:boolean\n",
    "            use user defined features or not\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    preprocess_data()\n",
    "        main method to do the feature engineering\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_technical_indicator=True,\n",
    "        tech_indicator_list=config.INDICATORS,\n",
    "        use_vix=False,\n",
    "        use_turbulence=False,\n",
    "        user_defined_feature=False,\n",
    "    ):\n",
    "        self.use_technical_indicator = use_technical_indicator\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.use_vix = use_vix\n",
    "        self.use_turbulence = use_turbulence\n",
    "        self.user_defined_feature = user_defined_feature\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"main method to do the feature engineering\n",
    "        @:param config: source dataframe\n",
    "        @:return: a DataMatrices object\n",
    "        \"\"\"\n",
    "        # clean data\n",
    "        df = self.clean_data(df)\n",
    "\n",
    "        # add technical indicators using stockstats\n",
    "        if self.use_technical_indicator:\n",
    "            df = self.add_technical_indicator(df)\n",
    "            print(\"Successfully added technical indicators\")\n",
    "\n",
    "        # add vix for multiple stock\n",
    "        if self.use_vix:\n",
    "            df = self.add_vix(df)\n",
    "            print(\"Successfully added vix\")\n",
    "\n",
    "        # add turbulence index for multiple stock\n",
    "        if self.use_turbulence:\n",
    "            df = self.add_turbulence(df)\n",
    "            print(\"Successfully added turbulence index\")\n",
    "\n",
    "        # add user defined feature\n",
    "        if self.user_defined_feature:\n",
    "            df = self.add_user_defined_feature(df)\n",
    "            print(\"Successfully added user defined features\")\n",
    "\n",
    "        # fill the missing values at the beginning and the end\n",
    "        df = df.ffill().bfill()\n",
    "        return df\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        \"\"\"\n",
    "        clean the raw data\n",
    "        deal with missing values\n",
    "        reasons: stocks could be delisted, not incorporated at the time step\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "        df.index = df.date.factorize()[0]\n",
    "        merged_closes = df.pivot_table(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        merged_closes = merged_closes.fillna(merged_closes.mean())\n",
    "        # merged_closes = merged_closes.fillna(merged_closes.mean())\n",
    "        tics = merged_closes.columns\n",
    "        df = df[df.tic.isin(tics)]\n",
    "        \n",
    "        # df = data.copy()\n",
    "        # list_ticker = df[\"tic\"].unique().tolist()\n",
    "        # # only apply to daily level data, need to fix for minute level\n",
    "        # list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "        # combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "        # df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "        # df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "        # df_full = df_full.sort_values(['date','tic'])\n",
    "        # df_full = df_full.fillna(0)\n",
    "        return df\n",
    "\n",
    "    def add_technical_indicator(self, data):\n",
    "        \"\"\"\n",
    "        calculate technical indicators\n",
    "        use stockstats package to add technical inidactors\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values(by=[\"tic\", \"date\"])\n",
    "        stock = Sdf.retype(df.copy())\n",
    "        unique_ticker = stock.tic.unique()\n",
    "\n",
    "        for indicator in self.tech_indicator_list:\n",
    "            indicator_df = pd.DataFrame()\n",
    "            for i in range(len(unique_ticker)):\n",
    "                try:\n",
    "                    temp_indicator = stock[stock.tic == unique_ticker[i]][indicator]\n",
    "                    temp_indicator = pd.DataFrame(temp_indicator)\n",
    "                    temp_indicator[\"tic\"] = unique_ticker[i]\n",
    "                    temp_indicator[\"date\"] = df[df.tic == unique_ticker[i]][\n",
    "                        \"date\"\n",
    "                    ].to_list()\n",
    "                    # indicator_df = indicator_df.append(\n",
    "                    #     temp_indicator, ignore_index=True\n",
    "                    # )\n",
    "                    indicator_df = pd.concat(\n",
    "                        [indicator_df, temp_indicator], axis=0, ignore_index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            df = df.merge(\n",
    "                indicator_df[[\"tic\", \"date\", indicator]], on=[\"tic\", \"date\"], how=\"left\"\n",
    "            )\n",
    "        df = df.sort_values(by=[\"date\", \"tic\"])\n",
    "        return df\n",
    "        # df = data.set_index(['date','tic']).sort_index()\n",
    "        # df = df.join(df.groupby(level=0, group_keys=False).apply(lambda x, y: Sdf.retype(x)[y], y=self.tech_indicator_list))\n",
    "        # return df.reset_index()\n",
    "\n",
    "    def add_user_defined_feature(self, data):\n",
    "        \"\"\"\n",
    "         add user defined features\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df[\"daily_return\"] = df.close.pct_change(1)\n",
    "        # df['return_lag_1']=df.close.pct_change(2)\n",
    "        # df['return_lag_2']=df.close.pct_change(3)\n",
    "        # df['return_lag_3']=df.close.pct_change(4)\n",
    "        # df['return_lag_4']=df.close.pct_change(5)\n",
    "        return df\n",
    "\n",
    "    def add_vix(self, data):\n",
    "        \"\"\"\n",
    "        add vix from yahoo finance\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df_vix = YahooDownloader(\n",
    "            start_date=df.date.min(), end_date=df.date.max(), ticker_list=[\"^VIX\"]\n",
    "        ).fetch_data()\n",
    "        vix = df_vix[[\"date\", \"close\"]]\n",
    "        vix.columns = [\"date\", \"vix\"]\n",
    "\n",
    "        df = df.merge(vix, on=\"date\")\n",
    "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def add_turbulence(self, data):\n",
    "        \"\"\"\n",
    "        add turbulence index from a precalcualted dataframe\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        turbulence_index = self.calculate_turbulence(df)\n",
    "        df = df.merge(turbulence_index, on=\"date\")\n",
    "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def calculate_turbulence(self, data):\n",
    "        \"\"\"calculate turbulence index based on dow 30\"\"\"\n",
    "        # can add other market assets\n",
    "        df = data.copy()\n",
    "        df_price_pivot = df.pivot(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        # use returns to calculate turbulence\n",
    "        df_price_pivot = df_price_pivot.pct_change()\n",
    "\n",
    "        unique_date = df.date.unique()\n",
    "        # start after a year\n",
    "        start = 252\n",
    "        turbulence_index = [0] * start\n",
    "        # turbulence_index = [0]\n",
    "        count = 0\n",
    "        for i in range(start, len(unique_date)):\n",
    "            current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
    "            # use one year rolling window to calcualte covariance\n",
    "            hist_price = df_price_pivot[\n",
    "                (df_price_pivot.index < unique_date[i])\n",
    "                & (df_price_pivot.index >= unique_date[i - 252])\n",
    "            ]\n",
    "            # Drop tickers which has number missing values more than the \"oldest\" ticker\n",
    "            filtered_hist_price = hist_price.iloc[\n",
    "                hist_price.isna().sum().min() :\n",
    "            ].dropna(axis=1)\n",
    "\n",
    "            cov_temp = filtered_hist_price.cov()\n",
    "            current_temp = current_price[[x for x in filtered_hist_price]] - np.mean(\n",
    "                filtered_hist_price, axis=0\n",
    "            )\n",
    "            # cov_temp = hist_price.cov()\n",
    "            # current_temp=(current_price - np.mean(hist_price,axis=0))\n",
    "\n",
    "            temp = current_temp.values.dot(np.linalg.pinv(cov_temp)).dot(\n",
    "                current_temp.values.T\n",
    "            )\n",
    "            if temp > 0:\n",
    "                count += 1\n",
    "                if count > 2:\n",
    "                    turbulence_temp = temp[0][0]\n",
    "                else:\n",
    "                    # avoid large outlier because of the calculation just begins\n",
    "                    turbulence_temp = 0\n",
    "            else:\n",
    "                turbulence_temp = 0\n",
    "            turbulence_index.append(turbulence_temp)\n",
    "        try:\n",
    "            turbulence_index = pd.DataFrame(\n",
    "                {\"date\": df_price_pivot.index, \"turbulence\": turbulence_index}\n",
    "            )\n",
    "        except ValueError:\n",
    "            raise Exception(\"Turbulence information could not be added.\")\n",
    "        return turbulence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638a5036-ce5b-45e4-be69-ef5f6a46da2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        open        high         low       close   volume  \\\n",
       "0  2009-01-02   90.750000   90.750000   88.550003   48.861801    19140   \n",
       "1  2009-01-02  105.800003  109.599998  103.459999   71.914917  4536215   \n",
       "2  2009-01-02  206.050003  210.500000  196.500000  158.413025    52648   \n",
       "3  2009-01-02   15.140000   15.800000   14.975000   13.401811   136590   \n",
       "4  2009-01-02    6.660000    6.970000    6.350000    2.746401   274220   \n",
       "\n",
       "             tic  day  \n",
       "0  ASIANPAINT.BO    4  \n",
       "1    AXISBANK.BO    4  \n",
       "2  BAJAJ-AUTO.BO    4  \n",
       "3  BAJAJFINSV.BO    4  \n",
       "4  BAJFINANCE.BO    4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febd40c0-9d74-44a1-9fcf-a3b6f7b7bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\AppData\\Local\\Temp\\ipykernel_8532\\50613336.py:222: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df_price_pivot = df_price_pivot.pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "from finrl.config import INDICATORS\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                      tech_indicator_list = INDICATORS,\n",
    "                      use_vix=False,\n",
    "                      use_turbulence=True,\n",
    "                      user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083563ce-5668-49e3-abed-4c42a4bfa9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105698</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>979.250000</td>\n",
       "      <td>986.799988</td>\n",
       "      <td>26056</td>\n",
       "      <td>SUNPHARMA.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.263414</td>\n",
       "      <td>1019.314408</td>\n",
       "      <td>965.265603</td>\n",
       "      <td>50.085294</td>\n",
       "      <td>14.481255</td>\n",
       "      <td>1.567920</td>\n",
       "      <td>983.446670</td>\n",
       "      <td>985.046100</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105699</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>3208.000000</td>\n",
       "      <td>3227.199951</td>\n",
       "      <td>3197.149902</td>\n",
       "      <td>3175.769043</td>\n",
       "      <td>51644</td>\n",
       "      <td>TCS.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-15.398183</td>\n",
       "      <td>3235.633708</td>\n",
       "      <td>3045.249324</td>\n",
       "      <td>48.649310</td>\n",
       "      <td>67.966063</td>\n",
       "      <td>0.407494</td>\n",
       "      <td>3131.238102</td>\n",
       "      <td>3257.234477</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105700</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>1026.650024</td>\n",
       "      <td>982.950012</td>\n",
       "      <td>986.955139</td>\n",
       "      <td>279514</td>\n",
       "      <td>TECHM.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-22.941437</td>\n",
       "      <td>1102.074200</td>\n",
       "      <td>929.293964</td>\n",
       "      <td>44.970681</td>\n",
       "      <td>-99.119890</td>\n",
       "      <td>22.233939</td>\n",
       "      <td>1033.226742</td>\n",
       "      <td>1032.633037</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105701</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2679.300049</td>\n",
       "      <td>2620.050049</td>\n",
       "      <td>2640.399902</td>\n",
       "      <td>32742</td>\n",
       "      <td>TITAN.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>43.161331</td>\n",
       "      <td>2669.834479</td>\n",
       "      <td>2492.325506</td>\n",
       "      <td>60.306098</td>\n",
       "      <td>116.653875</td>\n",
       "      <td>37.463255</td>\n",
       "      <td>2542.391650</td>\n",
       "      <td>2482.099988</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105702</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>7524.950195</td>\n",
       "      <td>7576.549805</td>\n",
       "      <td>7487.549805</td>\n",
       "      <td>7520.226562</td>\n",
       "      <td>8316</td>\n",
       "      <td>ULTRACEMCO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>28.692164</td>\n",
       "      <td>7774.630825</td>\n",
       "      <td>7303.703648</td>\n",
       "      <td>55.649892</td>\n",
       "      <td>27.765845</td>\n",
       "      <td>3.677975</td>\n",
       "      <td>7441.862826</td>\n",
       "      <td>7301.539185</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105703 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date         open         high          low        close  \\\n",
       "0       2009-01-02    90.750000    90.750000    88.550003    48.861801   \n",
       "1       2009-01-02   105.800003   109.599998   103.459999    71.914917   \n",
       "2       2009-01-02   206.050003   210.500000   196.500000   158.413025   \n",
       "3       2009-01-02    15.140000    15.800000    14.975000    13.401811   \n",
       "4       2009-01-02     6.660000     6.970000     6.350000     2.746401   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "105698  2023-04-28   981.000000   992.500000   979.250000   986.799988   \n",
       "105699  2023-04-28  3208.000000  3227.199951  3197.149902  3175.769043   \n",
       "105700  2023-04-28   983.000000  1026.650024   982.950012   986.955139   \n",
       "105701  2023-04-28  2663.500000  2679.300049  2620.050049  2640.399902   \n",
       "105702  2023-04-28  7524.950195  7576.549805  7487.549805  7520.226562   \n",
       "\n",
       "         volume            tic  day       macd      boll_ub      boll_lb  \\\n",
       "0         19140  ASIANPAINT.BO    4   0.000000    50.523346    48.068260   \n",
       "1       4536215    AXISBANK.BO    4   0.000000    50.523346    48.068260   \n",
       "2         52648  BAJAJ-AUTO.BO    4   0.000000    50.523346    48.068260   \n",
       "3        136590  BAJAJFINSV.BO    4   0.000000    50.523346    48.068260   \n",
       "4        274220  BAJFINANCE.BO    4   0.000000    50.523346    48.068260   \n",
       "...         ...            ...  ...        ...          ...          ...   \n",
       "105698    26056   SUNPHARMA.BO    4  -0.263414  1019.314408   965.265603   \n",
       "105699    51644         TCS.BO    4 -15.398183  3235.633708  3045.249324   \n",
       "105700   279514       TECHM.BO    4 -22.941437  1102.074200   929.293964   \n",
       "105701    32742       TITAN.BO    4  43.161331  2669.834479  2492.325506   \n",
       "105702     8316  ULTRACEMCO.BO    4  28.692164  7774.630825  7303.703648   \n",
       "\n",
       "            rsi_30      cci_30       dx_30  close_30_sma  close_60_sma  \\\n",
       "0       100.000000   66.666667  100.000000     48.861801     48.861801   \n",
       "1       100.000000   66.666667  100.000000     71.914917     71.914917   \n",
       "2       100.000000   66.666667  100.000000    158.413025    158.413025   \n",
       "3       100.000000   66.666667  100.000000     13.401811     13.401811   \n",
       "4       100.000000   66.666667  100.000000      2.746401      2.746401   \n",
       "...            ...         ...         ...           ...           ...   \n",
       "105698   50.085294   14.481255    1.567920    983.446670    985.046100   \n",
       "105699   48.649310   67.966063    0.407494   3131.238102   3257.234477   \n",
       "105700   44.970681  -99.119890   22.233939   1033.226742   1032.633037   \n",
       "105701   60.306098  116.653875   37.463255   2542.391650   2482.099988   \n",
       "105702   55.649892   27.765845    3.677975   7441.862826   7301.539185   \n",
       "\n",
       "        turbulence  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "...            ...  \n",
       "105698   43.069415  \n",
       "105699   43.069415  \n",
       "105700   43.069415  \n",
       "105701   43.069415  \n",
       "105702   43.069415  \n",
       "\n",
       "[105703 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b299a545-307a-4d24-a530-9c7a1e0e84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b45789e-5f8a-4a0d-a1ba-070d4601b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = df[\"tic\"].unique().tolist()\n",
    "# only apply to daily level data, need to fix for minute level\n",
    "list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "df_full = df_full.sort_values(['date','tic'])\n",
    "df_full = df_full.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4ccb1a-fe6a-41e1-95fe-8a3363465c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 105900 entries, 0 to 156899\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   date          105900 non-null  object \n",
      " 1   tic           105900 non-null  object \n",
      " 2   open          105900 non-null  float64\n",
      " 3   high          105900 non-null  float64\n",
      " 4   low           105900 non-null  float64\n",
      " 5   close         105900 non-null  float64\n",
      " 6   volume        105900 non-null  float64\n",
      " 7   day           105900 non-null  float64\n",
      " 8   macd          105900 non-null  float64\n",
      " 9   boll_ub       105900 non-null  float64\n",
      " 10  boll_lb       105900 non-null  float64\n",
      " 11  rsi_30        105900 non-null  float64\n",
      " 12  cci_30        105900 non-null  float64\n",
      " 13  dx_30         105900 non-null  float64\n",
      " 14  close_30_sma  105900 non-null  float64\n",
      " 15  close_60_sma  105900 non-null  float64\n",
      " 16  turbulence    105900 non-null  float64\n",
      "dtypes: float64(15), object(2)\n",
      "memory usage: 14.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f2577b7-eabd-4f2c-a467-cb5584c94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05041644-b750-457e-81c5-29c56fc1d53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tic        open        high         low       close  \\\n",
       "0  2009-01-02  ASIANPAINT.BO   90.750000   90.750000   88.550003   48.861801   \n",
       "1  2009-01-02    AXISBANK.BO  105.800003  109.599998  103.459999   71.914917   \n",
       "2  2009-01-02  BAJAJ-AUTO.BO  206.050003  210.500000  196.500000  158.413025   \n",
       "3  2009-01-02  BAJAJFINSV.BO   15.140000   15.800000   14.975000   13.401811   \n",
       "4  2009-01-02  BAJFINANCE.BO    6.660000    6.970000    6.350000    2.746401   \n",
       "\n",
       "      volume  day  macd    boll_ub   boll_lb  rsi_30     cci_30  dx_30  \\\n",
       "0    19140.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "1  4536215.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "2    52648.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "3   136590.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "4   274220.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "\n",
       "   close_30_sma  close_60_sma  turbulence  \n",
       "0     48.861801     48.861801         0.0  \n",
       "1     71.914917     71.914917         0.0  \n",
       "2    158.413025    158.413025         0.0  \n",
       "3     13.401811     13.401811         0.0  \n",
       "4      2.746401      2.746401         0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067ab770-a250-4ae6-9192-79a47e011ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105900, 17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8ef628-f80f-4d6d-9a2f-b626ad01064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# from stable_baselines3.common.logger import Logger, KVWriter, CSVOutputFormat\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        stock_dim: int,\n",
    "        hmax: int,\n",
    "        initial_amount: int,\n",
    "        num_stock_shares: list[int],\n",
    "        buy_cost_pct: list[float],\n",
    "        sell_cost_pct: list[float],\n",
    "        reward_scaling: float,\n",
    "        state_space: int,\n",
    "        action_space: int,\n",
    "        tech_indicator_list: list[str],\n",
    "        turbulence_threshold=None,\n",
    "        risk_indicator_col=\"turbulence\",\n",
    "        make_plots: bool = False,\n",
    "        print_verbosity=10,\n",
    "        day=0,\n",
    "        initial=True,\n",
    "        previous_state=[],\n",
    "        model_name=\"\",\n",
    "        mode=\"\",\n",
    "        iteration=\"\",\n",
    "    ):\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.num_stock_shares = num_stock_shares\n",
    "        self.initial_amount = initial_amount  # get the initial cash\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.state_space,)\n",
    "        )\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.terminal = False\n",
    "        self.make_plots = make_plots\n",
    "        self.print_verbosity = print_verbosity\n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        self.risk_indicator_col = risk_indicator_col\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.iteration = iteration\n",
    "        # initalize state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.episode = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [\n",
    "            self.initial_amount\n",
    "            + np.sum(\n",
    "                np.array(self.num_stock_shares)\n",
    "                * np.array(self.state[1 : 1 + self.stock_dim])\n",
    "            )\n",
    "        ]  # the initial total asset is calculated by cash + sum (num_share_stock_i * price_stock_i)\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.state_memory = (\n",
    "            []\n",
    "        )  # we need sometimes to preserve the state in the middle of trading process\n",
    "        self.date_memory = [self._get_date()]\n",
    "        #         self.logger = Logger('results',[CSVOutputFormat])\n",
    "        # self.reset()\n",
    "        self._seed()\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        def _do_sell_normal():\n",
    "            if (\n",
    "                self.state[index + 2 * self.stock_dim + 1] != True\n",
    "            ):  # check if the stock is able to sell, for simlicity we just add it in techical index\n",
    "                # if self.state[index + 1] > 0: # if we use price<0 to denote a stock is unable to trade in that day, the total asset calculation may be wrong for the price is unreasonable\n",
    "                # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                # perform sell action based on the sign of the action\n",
    "                if self.state[index + self.stock_dim + 1] > 0:\n",
    "                    # Sell only if current asset is > 0\n",
    "                    sell_num_shares = min(\n",
    "                        abs(action), self.state[index + self.stock_dim + 1]\n",
    "                    )\n",
    "                    sell_amount = (\n",
    "                        self.state[index + 1]\n",
    "                        * sell_num_shares\n",
    "                        * (1 - self.sell_cost_pct[index])\n",
    "                    )\n",
    "                    # update balance\n",
    "                    self.state[0] += sell_amount\n",
    "\n",
    "                    self.state[index + self.stock_dim + 1] -= sell_num_shares\n",
    "                    self.cost += (\n",
    "                        self.state[index + 1]\n",
    "                        * sell_num_shares\n",
    "                        * self.sell_cost_pct[index]\n",
    "                    )\n",
    "                    self.trades += 1\n",
    "                else:\n",
    "                    sell_num_shares = 0\n",
    "            else:\n",
    "                sell_num_shares = 0\n",
    "\n",
    "            return sell_num_shares\n",
    "\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence_threshold is not None:\n",
    "            if self.turbulence >= self.turbulence_threshold:\n",
    "                if self.state[index + 1] > 0:\n",
    "                    # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                    # if turbulence goes over threshold, just clear out all positions\n",
    "                    if self.state[index + self.stock_dim + 1] > 0:\n",
    "                        # Sell only if current asset is > 0\n",
    "                        sell_num_shares = self.state[index + self.stock_dim + 1]\n",
    "                        sell_amount = (\n",
    "                            self.state[index + 1]\n",
    "                            * sell_num_shares\n",
    "                            * (1 - self.sell_cost_pct[index])\n",
    "                        )\n",
    "                        # update balance\n",
    "                        self.state[0] += sell_amount\n",
    "                        self.state[index + self.stock_dim + 1] = 0\n",
    "                        self.cost += (\n",
    "                            self.state[index + 1]\n",
    "                            * sell_num_shares\n",
    "                            * self.sell_cost_pct[index]\n",
    "                        )\n",
    "                        self.trades += 1\n",
    "                    else:\n",
    "                        sell_num_shares = 0\n",
    "                else:\n",
    "                    sell_num_shares = 0\n",
    "            else:\n",
    "                sell_num_shares = _do_sell_normal()\n",
    "        else:\n",
    "            sell_num_shares = _do_sell_normal()\n",
    "\n",
    "        return sell_num_shares\n",
    "\n",
    "    def _buy_stock(self, index, action):\n",
    "        def _do_buy():\n",
    "            if (\n",
    "                self.state[index + 2 * self.stock_dim + 1] != True\n",
    "            ):  # check if the stock is able to buy\n",
    "                # if self.state[index + 1] >0:\n",
    "                # Buy only if the price is > 0 (no missing data in this particular date)\n",
    "                available_amount = self.state[0] // (\n",
    "                    self.state[index + 1] * (1 + self.buy_cost_pct[index])\n",
    "                )  # when buying stocks, we should consider the cost of trading when calculating available_amount, or we may be have cash<0\n",
    "                # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "                # update balance\n",
    "                buy_num_shares = min(available_amount, action)\n",
    "                buy_amount = (\n",
    "                    self.state[index + 1]\n",
    "                    * buy_num_shares\n",
    "                    * (1 + self.buy_cost_pct[index])\n",
    "                )\n",
    "                self.state[0] -= buy_amount\n",
    "\n",
    "                self.state[index + self.stock_dim + 1] += buy_num_shares\n",
    "\n",
    "                self.cost += (\n",
    "                    self.state[index + 1] * buy_num_shares * self.buy_cost_pct[index]\n",
    "                )\n",
    "                self.trades += 1\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "\n",
    "            return buy_num_shares\n",
    "\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence_threshold is None:\n",
    "            buy_num_shares = _do_buy()\n",
    "        else:\n",
    "            if self.turbulence < self.turbulence_threshold:\n",
    "                buy_num_shares = _do_buy()\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "                pass\n",
    "\n",
    "        return buy_num_shares\n",
    "\n",
    "    def _make_plot(self):\n",
    "        plt.plot(self.asset_memory, \"r\")\n",
    "        plt.savefig(f\"results/account_value_trade_{self.episode}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "        if self.terminal:\n",
    "            # print(f\"Episode: {self.episode}\")\n",
    "            if self.make_plots:\n",
    "                self._make_plot()\n",
    "            end_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            tot_reward = (\n",
    "                self.state[0]\n",
    "                + sum(\n",
    "                    np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                    * np.array(\n",
    "                        self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
    "                    )\n",
    "                )\n",
    "                - self.asset_memory[0]\n",
    "            )  # initial_amount is only cash part of our initial asset\n",
    "            df_total_value.columns = [\"account_value\"]\n",
    "            df_total_value[\"date\"] = self.date_memory\n",
    "            df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(\n",
    "                1\n",
    "            )\n",
    "            if df_total_value[\"daily_return\"].std() != 0:\n",
    "                sharpe = (\n",
    "                    (252**0.5)\n",
    "                    * df_total_value[\"daily_return\"].mean()\n",
    "                    / df_total_value[\"daily_return\"].std()\n",
    "                )\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            df_rewards.columns = [\"account_rewards\"]\n",
    "            df_rewards[\"date\"] = self.date_memory[:-1]\n",
    "            if self.episode % self.print_verbosity == 0:\n",
    "                print(f\"day: {self.day}, episode: {self.episode}\")\n",
    "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
    "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
    "                print(f\"total_reward: {tot_reward:0.2f}\")\n",
    "                print(f\"total_cost: {self.cost:0.2f}\")\n",
    "                print(f\"total_trades: {self.trades}\")\n",
    "                if df_total_value[\"daily_return\"].std() != 0:\n",
    "                    print(f\"Sharpe: {sharpe:0.3f}\")\n",
    "                print(\"=================================\")\n",
    "\n",
    "            if (self.model_name != \"\") and (self.mode != \"\"):\n",
    "                df_actions = self.save_action_memory()\n",
    "                df_actions.to_csv(\n",
    "                    \"results/actions_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    )\n",
    "                )\n",
    "                df_total_value.to_csv(\n",
    "                    \"results/account_value_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                df_rewards.to_csv(\n",
    "                    \"results/account_rewards_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                plt.plot(self.asset_memory, \"r\")\n",
    "                plt.savefig(\n",
    "                    \"results/account_value_{}_{}_{}.png\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    )\n",
    "                )\n",
    "                plt.close()\n",
    "\n",
    "            # Add outputs to logger interface\n",
    "            # logger.record(\"environment/portfolio_value\", end_total_asset)\n",
    "            # logger.record(\"environment/total_reward\", tot_reward)\n",
    "            # logger.record(\"environment/total_reward_pct\", (tot_reward / (end_total_asset - tot_reward)) * 100)\n",
    "            # logger.record(\"environment/total_cost\", self.cost)\n",
    "            # logger.record(\"environment/total_trades\", self.trades)\n",
    "\n",
    "            return self.state, self.reward, self.terminal, False, {}\n",
    "\n",
    "        else:\n",
    "            actions = actions * self.hmax  # actions initially is scaled between 0 to 1\n",
    "            actions = actions.astype(\n",
    "                int\n",
    "            )  # convert into integer because we can't by fraction of shares\n",
    "            if self.turbulence_threshold is not None:\n",
    "                if self.turbulence >= self.turbulence_threshold:\n",
    "                    actions = np.array([-self.hmax] * self.stock_dim)\n",
    "            begin_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            # print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions)\n",
    "            sell_index = argsort_actions[: np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][: np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print(f\"Num shares before: {self.state[index+self.stock_dim+1]}\")\n",
    "                # print(f'take sell action before : {actions[index]}')\n",
    "                actions[index] = self._sell_stock(index, actions[index]) * (-1)\n",
    "                # print(f'take sell action after : {actions[index]}')\n",
    "                # print(f\"Num shares after: {self.state[index+self.stock_dim+1]}\")\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                actions[index] = self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.actions_memory.append(actions)\n",
    "\n",
    "            # state: s -> s+1\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            if self.turbulence_threshold is not None:\n",
    "                if len(self.df.tic.unique()) == 1:\n",
    "                    self.turbulence = self.data[self.risk_indicator_col]\n",
    "                elif len(self.df.tic.unique()) > 1:\n",
    "                    self.turbulence = self.data[self.risk_indicator_col].values[0]\n",
    "            self.state = self._update_state()\n",
    "\n",
    "            end_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            self.date_memory.append(self._get_date())\n",
    "            self.reward = end_total_asset - begin_total_asset\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward * self.reward_scaling\n",
    "            self.state_memory.append(\n",
    "                self.state\n",
    "            )  # add current state in state_recorder for each step\n",
    "\n",
    "        return self.state, self.reward, self.terminal, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed=None,\n",
    "        options=None,\n",
    "    ):\n",
    "        # initiate state\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        if self.initial:\n",
    "            self.asset_memory = [\n",
    "                self.initial_amount\n",
    "                + np.sum(\n",
    "                    np.array(self.num_stock_shares)\n",
    "                    * np.array(self.state[1 : 1 + self.stock_dim])\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            previous_total_asset = self.previous_state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(\n",
    "                    self.previous_state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
    "                )\n",
    "            )\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False\n",
    "        # self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "\n",
    "        self.episode += 1\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        return self.state\n",
    "\n",
    "    def _initiate_state(self):\n",
    "        if self.initial:\n",
    "            # For Initial State\n",
    "            if len(self.df.tic.unique()) > 1:\n",
    "                # for multiple stock\n",
    "                state = (\n",
    "                    [self.initial_amount]\n",
    "                    + self.data.close.values.tolist()\n",
    "                    + self.num_stock_shares\n",
    "                    + sum(\n",
    "                        (\n",
    "                            self.data[tech].values.tolist()\n",
    "                            for tech in self.tech_indicator_list\n",
    "                        ),\n",
    "                        [],\n",
    "                    )\n",
    "                )  # append initial stocks_share to initial state, instead of all zero\n",
    "            else:\n",
    "                # for single stock\n",
    "                state = (\n",
    "                    [self.initial_amount]\n",
    "                    + [self.data.close]\n",
    "                    + [0] * self.stock_dim\n",
    "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "                )\n",
    "        else:\n",
    "            # Using Previous State\n",
    "            if len(self.df.tic.unique()) > 1:\n",
    "                # for multiple stock\n",
    "                state = (\n",
    "                    [self.previous_state[0]]\n",
    "                    + self.data.close.values.tolist()\n",
    "                    + self.previous_state[\n",
    "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
    "                    ]\n",
    "                    + sum(\n",
    "                        (\n",
    "                            self.data[tech].values.tolist()\n",
    "                            for tech in self.tech_indicator_list\n",
    "                        ),\n",
    "                        [],\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # for single stock\n",
    "                state = (\n",
    "                    [self.previous_state[0]]\n",
    "                    + [self.data.close]\n",
    "                    + self.previous_state[\n",
    "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
    "                    ]\n",
    "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "                )\n",
    "        return state\n",
    "\n",
    "    def _update_state(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # for multiple stock\n",
    "            state = (\n",
    "                [self.state[0]]\n",
    "                + self.data.close.values.tolist()\n",
    "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "                + sum(\n",
    "                    (\n",
    "                        self.data[tech].values.tolist()\n",
    "                        for tech in self.tech_indicator_list\n",
    "                    ),\n",
    "                    [],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # for single stock\n",
    "            state = (\n",
    "                [self.state[0]]\n",
    "                + [self.data.close]\n",
    "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "                + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "            )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _get_date(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            date = self.data.date.unique()[0]\n",
    "        else:\n",
    "            date = self.data.date\n",
    "        return date\n",
    "\n",
    "    # add save_state_memory to preserve state in the trading process\n",
    "    def save_state_memory(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # date and close price length must match actions length\n",
    "            date_list = self.date_memory[:-1]\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            state_list = self.state_memory\n",
    "            df_states = pd.DataFrame(\n",
    "                state_list,\n",
    "                columns=[\n",
    "                    \"cash\",\n",
    "                    \"Bitcoin_price\",\n",
    "                    \"Gold_price\",\n",
    "                    \"Bitcoin_num\",\n",
    "                    \"Gold_num\",\n",
    "                    \"Bitcoin_Disable\",\n",
    "                    \"Gold_Disable\",\n",
    "                ],\n",
    "            )\n",
    "            df_states.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        else:\n",
    "            date_list = self.date_memory[:-1]\n",
    "            state_list = self.state_memory\n",
    "            df_states = pd.DataFrame({\"date\": date_list, \"states\": state_list})\n",
    "        # print(df_states)\n",
    "        return df_states\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        asset_list = self.asset_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame(\n",
    "            {\"date\": date_list, \"account_value\": asset_list}\n",
    "        )\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # date and close price length must match actions length\n",
    "            date_list = self.date_memory[:-1]\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            action_list = self.actions_memory\n",
    "            df_actions = pd.DataFrame(action_list)\n",
    "            df_actions.columns = self.data.tic.values\n",
    "            df_actions.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        else:\n",
    "            date_list = self.date_memory[:-1]\n",
    "            action_list = self.actions_memory\n",
    "            df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbe291-9e9d-4028-8ef4-897e83cf2318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d66621-48cf-40ea-b98d-5b262d41fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77550\n",
      "21120\n"
     ]
    }
   ],
   "source": [
    "train = data_split(df, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(df, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76538de6-2554-4a7f-b69b-7f1303363465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cce40-87e6-410c-ac31-d555f17adaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db88fde3-3088-471d-a275-f30f1c22f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 301\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "490ac2e0-422b-4f0d-9d59-c7a13738bb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>179.990005</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>113.311302</td>\n",
       "      <td>26700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.051069</td>\n",
       "      <td>115.089549</td>\n",
       "      <td>104.640905</td>\n",
       "      <td>66.436248</td>\n",
       "      <td>113.218646</td>\n",
       "      <td>24.458130</td>\n",
       "      <td>108.824595</td>\n",
       "      <td>104.144379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>197.600006</td>\n",
       "      <td>142.226456</td>\n",
       "      <td>658270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.338474</td>\n",
       "      <td>151.029051</td>\n",
       "      <td>132.164238</td>\n",
       "      <td>52.578761</td>\n",
       "      <td>-1.919545</td>\n",
       "      <td>2.331440</td>\n",
       "      <td>142.339130</td>\n",
       "      <td>140.995590</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>687.819336</td>\n",
       "      <td>71150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.282086</td>\n",
       "      <td>707.370571</td>\n",
       "      <td>641.819761</td>\n",
       "      <td>61.119288</td>\n",
       "      <td>90.337046</td>\n",
       "      <td>15.068862</td>\n",
       "      <td>655.815855</td>\n",
       "      <td>622.730791</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>34.900002</td>\n",
       "      <td>36.080002</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>31.331820</td>\n",
       "      <td>1119010.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>30.449640</td>\n",
       "      <td>27.363217</td>\n",
       "      <td>60.736230</td>\n",
       "      <td>321.150339</td>\n",
       "      <td>46.461213</td>\n",
       "      <td>28.700716</td>\n",
       "      <td>28.163338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>34.389999</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>16.682077</td>\n",
       "      <td>221680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568160</td>\n",
       "      <td>16.397749</td>\n",
       "      <td>14.096924</td>\n",
       "      <td>68.269383</td>\n",
       "      <td>212.436642</td>\n",
       "      <td>27.581496</td>\n",
       "      <td>14.887369</td>\n",
       "      <td>14.188843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tic        open        high         low       close  \\\n",
       "0  2010-01-04  ASIANPAINT.BO  173.800003  179.990005  173.800003  113.311302   \n",
       "0  2010-01-04    AXISBANK.BO  199.800003  199.800003  197.600006  142.226456   \n",
       "0  2010-01-04  BAJAJ-AUTO.BO  885.000000  886.000000  865.000000  687.819336   \n",
       "0  2010-01-04  BAJAJFINSV.BO   34.900002   36.080002   34.799999   31.331820   \n",
       "0  2010-01-04  BAJFINANCE.BO   33.270000   34.389999   33.270000   16.682077   \n",
       "\n",
       "      volume  day       macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "0    26700.0  0.0   2.051069  115.089549  104.640905  66.436248  113.218646   \n",
       "0   658270.0  0.0  -0.338474  151.029051  132.164238  52.578761   -1.919545   \n",
       "0    71150.0  0.0  16.282086  707.370571  641.819761  61.119288   90.337046   \n",
       "0  1119010.0  0.0   0.473058   30.449640   27.363217  60.736230  321.150339   \n",
       "0   221680.0  0.0   0.568160   16.397749   14.096924  68.269383  212.436642   \n",
       "\n",
       "       dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "0  24.458130    108.824595    104.144379         0.0  \n",
       "0   2.331440    142.339130    140.995590         0.0  \n",
       "0  15.068862    655.815855    622.730791         0.0  \n",
       "0  46.461213     28.700716     28.163338         0.0  \n",
       "0  27.581496     14.887369     14.188843         0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96cf6369-0195-453c-be44-674a48ac5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd613e04-ff28-40c4-8eba-fa0fcafdb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da8507ba-f46f-43df-bc16-9ad39026c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7d88eb5-3fdf-4d97-87e6-6fac63e83859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from stable_baselines3.common.logger import configure\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01cfb65-c9c6-4920-a99e-4fb4cb34451b",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90aaf254-10a4-427b-87d0-a51677ae73e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81600ba0-a53e-4def-9010-6fd138f496a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 54          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -101        |\n",
      "|    reward             | -0.12433443 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 9.18        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 309       |\n",
      "|    reward             | 10.609722 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 61        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 387       |\n",
      "|    reward             | 1.6635258 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 88.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -1.02e+03 |\n",
      "|    reward             | 59.3306   |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 786       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -111       |\n",
      "|    reward             | -50.899265 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 169        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.000573 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -200      |\n",
      "|    reward             | 10.529587 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 26.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 491       |\n",
      "|    reward             | 1.9788064 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 145       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 399       |\n",
      "|    reward             | -3.929134 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 152       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -1.34e+03 |\n",
      "|    reward             | 7.5825224 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.4e+03   |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -28.6       |\n",
      "|    reward             | -0.14495529 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 77.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 2.58      |\n",
      "|    reward             | 2.4874575 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.00489    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -13.9      |\n",
      "|    reward             | -1.6160152 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.272      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -7.29      |\n",
      "|    reward             | 0.51229817 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 34.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 633       |\n",
      "|    reward             | 2.2995489 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 244       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 40.4       |\n",
      "|    reward             | -5.8517947 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 96.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 142        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -18.7      |\n",
      "|    reward             | -1.2739156 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.781      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 127       |\n",
      "|    reward             | 2.0266216 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 15.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 159        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 150        |\n",
      "|    reward             | 0.54670495 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 37.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 168        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 35.2       |\n",
      "|    reward             | -3.0863407 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 6.5        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -95.6    |\n",
      "|    reward             | 9.140593 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 24.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 186        |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.00501   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -50.9      |\n",
      "|    reward             | 0.10531089 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 2.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.00754    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 35.8       |\n",
      "|    reward             | 0.01283036 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 204       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0.0173    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 207       |\n",
      "|    reward             | 1.5766114 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 29.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 213        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 120        |\n",
      "|    reward             | -5.5822105 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 16.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 222        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 237        |\n",
      "|    reward             | -2.0644019 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 46.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 231        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | -140       |\n",
      "|    reward             | 0.44149247 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 240        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 46.6       |\n",
      "|    reward             | -1.0194371 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 2.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 248       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -122      |\n",
      "|    reward             | -4.618404 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 9.7       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 257         |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -16.5       |\n",
      "|    reward             | -0.34152722 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 2.96        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 266      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 2.1e-05  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -553     |\n",
      "|    reward             | 5.488657 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 206      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 275        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -207       |\n",
      "|    reward             | -7.8921676 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 96.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 284       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -130      |\n",
      "|    reward             | 4.3263617 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 19.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 293        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -115       |\n",
      "|    reward             | -3.0784006 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 14         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 301        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -206       |\n",
      "|    reward             | -1.0247914 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 47.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 310       |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 492       |\n",
      "|    reward             | 3.9078445 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 159       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 319       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -729      |\n",
      "|    reward             | 0.6183373 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 385       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 328        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -135       |\n",
      "|    reward             | -0.6556799 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 9.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 337       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -34.8     |\n",
      "|    reward             | -5.022275 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 7.33      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 345       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -339      |\n",
      "|    reward             | 2.1438015 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 66.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 354       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 143       |\n",
      "|    reward             | -3.138871 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 26.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 363       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | 254       |\n",
      "|    reward             | -9.170855 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 77.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 372      |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 54       |\n",
      "|    reward             | 1.372853 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 3.32     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 381        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 109        |\n",
      "|    reward             | -3.7117517 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 6.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 390       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 85.8      |\n",
      "|    reward             | -7.403728 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 7.87      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.000182  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 47.1       |\n",
      "|    reward             | -6.9158096 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.37       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 407        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 499        |\n",
      "|    reward             | 0.88637966 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 243        |\n",
      "--------------------------------------\n",
      "day: 2584, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5091309.26\n",
      "total_reward: 4091309.26\n",
      "total_cost: 4456.59\n",
      "total_trades: 41142\n",
      "Sharpe: 0.336\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 416       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -108      |\n",
      "|    reward             | -2.204816 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 425       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 22.3      |\n",
      "|    reward             | 2.8523772 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.56      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 434       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 200       |\n",
      "|    reward             | 7.3216376 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 46.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 443      |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 3.68     |\n",
      "|    reward             | -2.0078  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 19.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 452        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 20.6       |\n",
      "|    reward             | 0.75850105 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 17.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 461       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -23.6     |\n",
      "|    reward             | 1.7698933 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.523     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 470       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 41.4      |\n",
      "|    reward             | 1.2318509 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.19      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 479       |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | 289       |\n",
      "|    reward             | 0.5824102 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 58.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 488        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 581        |\n",
      "|    reward             | -10.867537 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 244        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 497       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -173      |\n",
      "|    reward             | -8.145756 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 185       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 506       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 38.2      |\n",
      "|    reward             | 0.3383966 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.71      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 515         |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | 17.2        |\n",
      "|    reward             | -0.40348163 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.584       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 524       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | 226       |\n",
      "|    reward             | 1.4917088 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 34.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 533       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 4.37      |\n",
      "|    reward             | 1.0132288 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 542       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -215      |\n",
      "|    reward             | -7.961758 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 551      |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 530      |\n",
      "|    reward             | 4.22605  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 261      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 560        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 9.67       |\n",
      "|    reward             | -0.8985198 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.7        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 569       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | -41.3     |\n",
      "|    reward             | -5.100491 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 578       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -30.1     |\n",
      "|    reward             | 2.1901805 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.13      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 587         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -172        |\n",
      "|    reward             | -0.19352044 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 19.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 595       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 203       |\n",
      "|    reward             | 10.711369 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 33.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 605        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 36.6       |\n",
      "|    reward             | 0.36602375 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.5        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 613       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -220      |\n",
      "|    reward             | -2.383693 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 32.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 622       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 82.1      |\n",
      "|    reward             | 4.5322385 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 631        |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | 144        |\n",
      "|    reward             | -1.6287004 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 17.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 640       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 337       |\n",
      "|    reward             | 39.692387 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 66.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 649       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -32       |\n",
      "|    reward             | 1.7530073 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 658      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 13       |\n",
      "|    reward             | 0.855717 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.418    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 666        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -529       |\n",
      "|    reward             | -4.2576103 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 174        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 675      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 215      |\n",
      "|    reward             | 4.803156 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 82       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 684       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -237      |\n",
      "|    reward             | 2.1119537 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 43        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 693        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -98.5      |\n",
      "|    reward             | -1.4210175 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 702        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -58.7      |\n",
      "|    reward             | -0.9686894 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 711       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -90.3     |\n",
      "|    reward             | 5.7672153 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 720       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -50.2     |\n",
      "|    reward             | 2.1896303 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.57      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 729       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 54.4      |\n",
      "|    reward             | 2.4455929 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 738       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -29.2     |\n",
      "|    reward             | -4.108154 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.538     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 746       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -73.5     |\n",
      "|    reward             | -0.566908 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 755       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -170      |\n",
      "|    reward             | 0.5541375 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 17.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 764        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 152        |\n",
      "|    reward             | -2.0869641 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 773      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 129      |\n",
      "|    reward             | 1.078538 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 25.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 782        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -47.2      |\n",
      "|    reward             | -0.8441565 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 791        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | -51.6      |\n",
      "|    reward             | -1.5228437 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.24       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 56          |\n",
      "|    iterations         | 9000        |\n",
      "|    time_elapsed       | 800         |\n",
      "|    total_timesteps    | 45000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8999        |\n",
      "|    policy_loss        | 129         |\n",
      "|    reward             | -0.21461883 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 9.38        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 809       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 612       |\n",
      "|    reward             | 4.3451443 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 216       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 818       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 1.31      |\n",
      "|    reward             | -6.890265 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.504     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 827      |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 187      |\n",
      "|    reward             | 5.286998 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 34.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 836        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | -144       |\n",
      "|    reward             | -1.4165533 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 845       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | -152      |\n",
      "|    reward             | 1.0150682 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 854       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | -178      |\n",
      "|    reward             | 2.8617036 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 23.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 863        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 273        |\n",
      "|    reward             | 0.43568057 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 44.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 871       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | -14.8     |\n",
      "|    reward             | 4.9028735 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.74      |\n",
      "-------------------------------------\n",
      "day: 2584, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4551450.29\n",
      "total_reward: 3551450.29\n",
      "total_cost: 2789.33\n",
      "total_trades: 37694\n",
      "Sharpe: 0.338\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 880        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 53.7       |\n",
      "|    reward             | 0.05750288 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 889        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 19.8       |\n",
      "|    reward             | -1.3429166 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.39       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3c468e6-ae7d-42a0-af6f-c0a6e7f3d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed82d86-faff-4008-846b-8097a00eb492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b36e7bc5-7e93-409f-9573-3c3f01cba845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
    "\n",
    "trained_a2c = A2C.load(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08afc9c2-5823-4afb-909a-7a9ab0622662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 301\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(trade.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4ad41c5-5076-4f57-bbc2-6649285e78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "250d6844-a243-4255-9abf-7eb6a471cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70, **env_kwargs)\n",
    "env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01104517-6131-43b1-aaf8-ac8fcd62f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c, \n",
    "    environment = e_trade_gym) if if_using_a2c else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d4354a4-3de3-4773-b7f3-9204c8f8bb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1891.650773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>1878.777787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>1887.697587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>1888.660050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>1886.457413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "699  2023-04-24    1891.650773\n",
       "700  2023-04-25    1878.777787\n",
       "701  2023-04-26    1887.697587\n",
       "702  2023-04-27    1888.660050\n",
       "703  2023-04-28    1886.457413"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_a2c.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c673516-c782-44d4-8d3d-d541f5c16863",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "790dae9c-e814-4736-a083-0a223218e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ba864a4-5815-4fa1-a0e9-e2f54be94915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 34        |\n",
      "|    time_elapsed    | 302       |\n",
      "|    total_timesteps | 10340     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 5.1       |\n",
      "|    critic_loss     | 11.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7755      |\n",
      "|    reward          | 3.0548356 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 662       |\n",
      "|    total_timesteps | 20680     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.28     |\n",
      "|    critic_loss     | 22.8      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 18095     |\n",
      "|    reward          | 3.0548356 |\n",
      "----------------------------------\n",
      "day: 2584, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4508581.73\n",
      "total_reward: 3508581.73\n",
      "total_cost: 998.99\n",
      "total_trades: 41287\n",
      "Sharpe: 0.429\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 1020      |\n",
      "|    total_timesteps | 31020     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.12     |\n",
      "|    critic_loss     | 4.43      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 28435     |\n",
      "|    reward          | 3.0548356 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 1377      |\n",
      "|    total_timesteps | 41360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -11       |\n",
      "|    critic_loss     | 2.52      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 38775     |\n",
      "|    reward          | 3.0548356 |\n",
      "----------------------------------\n",
      "day: 2584, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4508581.73\n",
      "total_reward: 3508581.73\n",
      "total_cost: 998.99\n",
      "total_trades: 41287\n",
      "Sharpe: 0.429\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 1736      |\n",
      "|    total_timesteps | 51700     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -11.7     |\n",
      "|    critic_loss     | 1.94      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 49115     |\n",
      "|    reward          | 3.0548356 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f867d1e0-85e4-469d-a490-3d35ccd15c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "231c819a-c423-4314-a3f5-3ad4c9e6aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg, \n",
    "    environment = e_trade_gym) if if_using_ddpg else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65d19496-cbdd-4623-9a5b-8e4bf294c4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>3787.118075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>3807.727252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>3803.319049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>3844.079135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>3811.998599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "699  2023-04-24    3787.118075\n",
       "700  2023-04-25    3807.727252\n",
       "701  2023-04-26    3803.319049\n",
       "702  2023-04-27    3844.079135\n",
       "703  2023-04-28    3811.998599"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_ddpg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4b2ad56-2037-45e2-a3a0-4f4e2dccf290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cb5f57d-f632-482e-9feb-0f0482565f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 76         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 26         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.8052117 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016108342 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0231     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.2         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    reward               | -1.6181117  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 37.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02329282 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.6      |\n",
      "|    explained_variance   | 0.00514    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 16         |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    reward               | 2.2154253  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 45.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019247364 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0409      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | -0.61891484 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025347814 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0446      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 41.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | -2.2035687  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021279624 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0669      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0076     |\n",
      "|    reward               | -0.7294496  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 64.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015739754 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0553      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 0.7543445   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 59.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021032535 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | 1.0632086   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 69.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019930867 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.0786      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 179         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    reward               | 0.24154319  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020273378 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.63        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    reward               | 2.3002853   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 46.6        |\n",
      "-----------------------------------------\n",
      "day: 2584, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1734412.84\n",
      "total_reward: 734412.84\n",
      "total_cost: 1660647.14\n",
      "total_trades: 61498\n",
      "Sharpe: 0.366\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023984026 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.099       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0083     |\n",
      "|    reward               | -0.33800292 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 71.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021783505 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.064       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | 4.1438785   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018908828 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 170         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    reward               | 0.31204966  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018335579 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.0911      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | -0.17252988 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 175         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030404378 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    reward               | 0.7479711   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 94.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 450        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02507151 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | 0.0495     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 20         |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    reward               | 1.5202549  |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 93.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030793333 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0498      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.3        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | 1.0452564   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 81.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 506         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019658085 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.0358      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00984    |\n",
      "|    reward               | -1.8268763  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021933373 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.0541      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    reward               | 0.49592873  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 66          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 562         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030815989 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.0677      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.5        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00927    |\n",
      "|    reward               | 1.3348013   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 79.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 590         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016850755 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0681      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00888    |\n",
      "|    reward               | -0.3217689  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013613664 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0428      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.6        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    reward               | -0.12401629 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 64.2        |\n",
      "-----------------------------------------\n",
      "day: 2584, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2667331.19\n",
      "total_reward: 1667331.19\n",
      "total_cost: 1640817.04\n",
      "total_trades: 60504\n",
      "Sharpe: 0.422\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 646         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013185287 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.0891      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    reward               | -1.8474051  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 86.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01648583  |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.9        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    reward               | -0.14709824 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012438882 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.5374362   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 95.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 729         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011141689 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 1.9710704   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 757         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012186502 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    reward               | -1.0240006  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 785         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015516229 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0532      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 533         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | -0.9504679  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 325         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019783938 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0727      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.5        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    reward               | 3.5114017   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 261         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 841         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014314178 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.0672      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 146         |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    reward               | 1.7094095   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 382         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 869         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012854025 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.0658      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.39        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    reward               | 0.52263594  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 898         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016065288 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.9        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 0.7856815   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 925          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015465338  |\n",
      "|    clip_fraction        | 0.186        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.3        |\n",
      "|    explained_variance   | 0.0866       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 236          |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    reward               | -0.040258802 |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 175          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 953         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017823668 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    reward               | 1.035115    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 58.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 981         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017497031 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    reward               | 0.95216507  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "day: 2584, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2064283.12\n",
      "total_reward: 1064283.12\n",
      "total_cost: 1554528.31\n",
      "total_trades: 59974\n",
      "Sharpe: 0.421\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1008        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014713533 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    reward               | 1.8361144   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1036        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015195389 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    reward               | 0.41925862  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 1063        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014026302 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | -0.89283484 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 1092       |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01444311 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.5      |\n",
      "|    explained_variance   | 0.259      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 9.08       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    reward               | -3.122002  |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 46.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 1120        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019855639 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 63          |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 0.8917873   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 46.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1147        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013761083 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    reward               | 0.62657136  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 52.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 1175       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02090573 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.6      |\n",
      "|    explained_variance   | 0.161      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 44.1       |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    reward               | -0.5154818 |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 135        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 1203        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015647275 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | 1.8027904   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 1230        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018980594 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.8        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | -3.204947   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 211         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1258        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019626364 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | -0.6925522  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1286        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012583816 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    reward               | 0.15480164  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 78.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 1313       |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01740089 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.9      |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 35.1       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    reward               | 1.616067   |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 128        |\n",
      "----------------------------------------\n",
      "day: 2584, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2542757.59\n",
      "total_reward: 1542757.59\n",
      "total_cost: 1586810.03\n",
      "total_trades: 61010\n",
      "Sharpe: 0.330\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1341        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023374759 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | -0.24042796 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 1368        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017793301 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 222         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    reward               | 1.4229051   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 1396        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017855868 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 3.952249    |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 1423        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020203965 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.7        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00517    |\n",
      "|    reward               | 2.983966    |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 96          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1450        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017523967 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.4        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | 1.2551202   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 194         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 1478        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018106313 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    reward               | 1.8910623   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 84.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1506        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014950338 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    reward               | 0.8018689   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 95.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1534        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014750489 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    reward               | 1.8499818   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 1562         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112291295 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.4        |\n",
      "|    explained_variance   | 0.179        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    reward               | -0.9151048   |\n",
      "|    std                  | 1.1          |\n",
      "|    value_loss           | 79.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1589        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014873544 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00948    |\n",
      "|    reward               | 2.0034359   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1617        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009626666 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 51          |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    reward               | -1.1610314  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 1645        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013823963 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    reward               | 0.74318576  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 1672        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013861856 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.8        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    reward               | -0.14052282 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "day: 2584, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4688028.05\n",
      "total_reward: 3688028.05\n",
      "total_cost: 1796220.72\n",
      "total_trades: 62230\n",
      "Sharpe: 0.347\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1699        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024503417 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -1.636865   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 140         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 62        |\n",
      "|    time_elapsed         | 1727      |\n",
      "|    total_timesteps      | 126976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0164036 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -45.6     |\n",
      "|    explained_variance   | 0.14      |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 40.8      |\n",
      "|    n_updates            | 610       |\n",
      "|    policy_gradient_loss | -0.0113   |\n",
      "|    reward               | 0.733903  |\n",
      "|    std                  | 1.11      |\n",
      "|    value_loss           | 207       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1754        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019678067 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    reward               | 2.465262    |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 1782        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011567226 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.8       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.6        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -1.8013796  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1810        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013846041 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.8       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.2        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | -1.2427891  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 174         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1837        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018044423 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 216         |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    reward               | -0.28788528 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1865        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020187005 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 77.5        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    reward               | 1.0064338   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 79.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1893        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019156229 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.6        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    reward               | 0.18973915  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 1920        |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012829859 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 41.9        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    reward               | 3.8047013   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1948        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012265857 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | -3.4496515  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 1975       |\n",
      "|    total_timesteps      | 145408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01481282 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.1      |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 63.1       |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    reward               | 0.9810203  |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 184        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 2003        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021632224 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.1       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 58          |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    reward               | -0.03715201 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 134         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 2031        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015906207 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -2.197584   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "day: 2584, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4063247.07\n",
      "total_reward: 3063247.07\n",
      "total_cost: 1776215.52\n",
      "total_trades: 62317\n",
      "Sharpe: 0.444\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 2058        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013025824 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 2.0896478   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 65.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 2087        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017517135 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56.9        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    reward               | -0.14985292 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 2115        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018582353 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.3       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 94.5        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00974    |\n",
      "|    reward               | -1.9901464  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 287         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 2143        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013504631 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.0748      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 157         |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    reward               | -0.5064748  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 445         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 2170        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017420733 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.0149      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 731         |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    reward               | -3.8330424  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 638         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 2197        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028386286 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 50          |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    reward               | 5.4476514   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 152         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 2225        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024546986 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.0139      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 288         |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00747    |\n",
      "|    reward               | 2.8911028   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 582         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 2253        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017743874 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.6       |\n",
      "|    explained_variance   | 0.0363      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 190         |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -1.6964644  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 732         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 2281       |\n",
      "|    total_timesteps      | 167936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0266015  |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.6      |\n",
      "|    explained_variance   | 0.167      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 228        |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.00944   |\n",
      "|    reward               | -1.0295501 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 162        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 2309        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017083816 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.7       |\n",
      "|    explained_variance   | 0.0475      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 179         |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | 5.841613    |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 523         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 2337        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022891179 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.8       |\n",
      "|    explained_variance   | 0.00729     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 285         |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    reward               | 0.50383425  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 668         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 2365        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027647322 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.8       |\n",
      "|    explained_variance   | 0.0393      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 134         |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    reward               | -3.2151334  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 277         |\n",
      "-----------------------------------------\n",
      "day: 2584, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7313204.39\n",
      "total_reward: 6313204.39\n",
      "total_cost: 1710474.63\n",
      "total_trades: 61243\n",
      "Sharpe: 0.315\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 2393        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028843617 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.8       |\n",
      "|    explained_variance   | 0.0401      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 201         |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    reward               | -2.0590997  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 450         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 2421       |\n",
      "|    total_timesteps      | 178176     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01426965 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.9      |\n",
      "|    explained_variance   | 0.0432     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 279        |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    reward               | 44.63267   |\n",
      "|    std                  | 1.16       |\n",
      "|    value_loss           | 1.19e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 2449        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018944636 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.9       |\n",
      "|    explained_variance   | 0.042       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 176         |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    reward               | 3.659453    |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 251         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 2477        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010575168 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47         |\n",
      "|    explained_variance   | 0.00767     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 397         |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | 1.7320902   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 561         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 2505        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016962778 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.1       |\n",
      "|    explained_variance   | 0.0145      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 695         |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00837    |\n",
      "|    reward               | 0.6352639   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 679         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 2532        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013263995 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.1       |\n",
      "|    explained_variance   | 0.089       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 315         |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    reward               | 0.8737182   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 284         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 2560       |\n",
      "|    total_timesteps      | 188416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01382801 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47.2      |\n",
      "|    explained_variance   | 0.0153     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 243        |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    reward               | 1.7574621  |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 438        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 2588        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013810757 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.2       |\n",
      "|    explained_variance   | 0.0338      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 178         |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00796    |\n",
      "|    reward               | 0.58008975  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 457         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 2616        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014703337 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.2       |\n",
      "|    explained_variance   | -0.00536    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 210         |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    reward               | 3.1188283   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 1.53e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 2643       |\n",
      "|    total_timesteps      | 194560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06692241 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47.2      |\n",
      "|    explained_variance   | 0.00697    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 851        |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | 0.00102    |\n",
      "|    reward               | 1.0006263  |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 2.08e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 2672        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030585859 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.0107      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.24e+03    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    reward               | -0.9440487  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 1.9e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 2699         |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0146632055 |\n",
      "|    clip_fraction        | 0.161        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.3        |\n",
      "|    explained_variance   | 0.0298       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 227          |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    reward               | 4.1110563    |\n",
      "|    std                  | 1.17         |\n",
      "|    value_loss           | 516          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 2727        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015676282 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    reward               | 0.62349445  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f1b6e13-9c3c-42a3-837e-9fed20675311",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59bc8368-a067-41da-a3c8-c01832ad8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_ppo, \n",
    "    environment = e_trade_gym) if if_using_ppo else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c28c6817-687a-43eb-b61e-82b4133797b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1761.592109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>1766.917213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>1771.709739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>1762.124518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>1749.344169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "699  2023-04-24    1761.592109\n",
       "700  2023-04-25    1766.917213\n",
       "701  2023-04-26    1771.709739\n",
       "702  2023-04-27    1762.124518\n",
       "703  2023-04-28    1749.344169"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_ppo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc99cbd1-137c-4b9d-89e8-7930d2554547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb8bb4bd-1de7-4b63-b1a7-8075c240d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 35        |\n",
      "|    time_elapsed    | 294       |\n",
      "|    total_timesteps | 10340     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 165       |\n",
      "|    critic_loss     | 156       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7755      |\n",
      "|    reward          | 1.6263274 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 640       |\n",
      "|    total_timesteps | 20680     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 129       |\n",
      "|    critic_loss     | 40.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 18095     |\n",
      "|    reward          | 1.6263274 |\n",
      "----------------------------------\n",
      "day: 2584, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4341565.60\n",
      "total_reward: 3341565.60\n",
      "total_cost: 1303.58\n",
      "total_trades: 41290\n",
      "Sharpe: 0.359\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 991       |\n",
      "|    total_timesteps | 31020     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 101       |\n",
      "|    critic_loss     | 20.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 28435     |\n",
      "|    reward          | 1.6263274 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 1275      |\n",
      "|    total_timesteps | 41360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 79.5      |\n",
      "|    critic_loss     | 17.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 38775     |\n",
      "|    reward          | 1.6263274 |\n",
      "----------------------------------\n",
      "day: 2584, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4341565.60\n",
      "total_reward: 3341565.60\n",
      "total_cost: 1303.58\n",
      "total_trades: 41290\n",
      "Sharpe: 0.359\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 1549      |\n",
      "|    total_timesteps | 51700     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 64.4      |\n",
      "|    critic_loss     | 8.98      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 49115     |\n",
      "|    reward          | 1.6263274 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93902222-3672-46b2-b507-709d244a357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c311428e-5efd-4687-a745-e4d63c46900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_td3, \n",
    "    environment = e_trade_gym) if if_using_td3 else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d9775b9-851d-44c0-a54c-690f8c72b765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>2527.550756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>2545.646261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>2571.674093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>2603.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>2663.133429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "699  2023-04-24    2527.550756\n",
       "700  2023-04-25    2545.646261\n",
       "701  2023-04-26    2571.674093\n",
       "702  2023-04-27    2603.001928\n",
       "703  2023-04-28    2663.133429"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_td3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34b43d95-5126-4723-9a66-185401b80ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_td3, \n",
    "    environment = e_trade_gym) if if_using_td3 else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6da0694-1230-40db-acf7-167680a0d2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>20432.454209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>20457.282441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>20555.994416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>20700.961411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>20905.607865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "699  2023-04-24   20432.454209\n",
       "700  2023-04-25   20457.282441\n",
       "701  2023-04-26   20555.994416\n",
       "702  2023-04-27   20700.961411\n",
       "703  2023-04-28   20905.607865"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_td3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3ab2b83-c8d2-440f-8390-9d65ab4e05bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebe25012-e974-4ff5-8edf-b1c39737f701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 395       |\n",
      "|    total_timesteps | 10340     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 828       |\n",
      "|    critic_loss     | 144       |\n",
      "|    ent_coef        | 0.217     |\n",
      "|    ent_coef_loss   | -4.17     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 10239     |\n",
      "|    reward          | 6.3860407 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 26       |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 20680    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 633      |\n",
      "|    critic_loss     | 73.2     |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -47.1    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 20579    |\n",
      "|    reward          | 9.619172 |\n",
      "---------------------------------\n",
      "day: 2584, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4417323.80\n",
      "total_reward: 3417323.80\n",
      "total_cost: 99060.42\n",
      "total_trades: 53238\n",
      "Sharpe: 0.403\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 26       |\n",
      "|    time_elapsed    | 1193     |\n",
      "|    total_timesteps | 31020    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 344      |\n",
      "|    critic_loss     | 58.5     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -141     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 30919    |\n",
      "|    reward          | 7.421438 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 1594      |\n",
      "|    total_timesteps | 41360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 208       |\n",
      "|    critic_loss     | 8.67      |\n",
      "|    ent_coef        | 0.0119    |\n",
      "|    ent_coef_loss   | -161      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 41259     |\n",
      "|    reward          | 3.8993683 |\n",
      "----------------------------------\n",
      "day: 2584, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3624095.11\n",
      "total_reward: 2624095.11\n",
      "total_cost: 16609.50\n",
      "total_trades: 47001\n",
      "Sharpe: 0.475\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 20         |\n",
      "|    fps             | 25         |\n",
      "|    time_elapsed    | 1991       |\n",
      "|    total_timesteps | 51700      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 128        |\n",
      "|    critic_loss     | 17.8       |\n",
      "|    ent_coef        | 0.00438    |\n",
      "|    ent_coef_loss   | -138       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 51599      |\n",
      "|    reward          | -11.230117 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 24         |\n",
      "|    fps             | 25         |\n",
      "|    time_elapsed    | 2466       |\n",
      "|    total_timesteps | 62040      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 96.8       |\n",
      "|    critic_loss     | 19.2       |\n",
      "|    ent_coef        | 0.00179    |\n",
      "|    ent_coef_loss   | -33.8      |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 61939      |\n",
      "|    reward          | -2.4988506 |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0ff242e-6727-4bf3-90aa-4686ed16682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99fea52c-38db-42e6-ad48-a9acbece3fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_sac, \n",
    "    environment = e_trade_gym) if if_using_sac else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66a7eb9d-c7df-4d37-b9f4-66c9466e6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>27203.938799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>27552.174883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>27780.241899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>27714.027910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>28356.541948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "699  2023-04-24   27203.938799\n",
       "700  2023-04-25   27552.174883\n",
       "701  2023-04-26   27780.241899\n",
       "702  2023-04-27   27714.027910\n",
       "703  2023-04-28   28356.541948"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_sac.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
